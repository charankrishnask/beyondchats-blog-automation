## ğŸ“˜ Phase Documentation

- [Phase 1 â€“ Backend & Scraping](./README.md)
- [Phase 2 â€“ Automated Article Enhancement](./src/phase2/README.md)
- Phase 3 â€“ React Frontend (In Progress)

ğŸ“˜ Phase 1 â€“ Blog Scraping & CRUD APIs
ğŸ“Œ Overview

Phase 1 focuses on building a backend system that scrapes blog articles from the BeyondChats Blogs section and exposes CRUD APIs to manage those articles.

This phase establishes the data foundation that will later be enhanced and automated using external sources and LLMs in Phase 2.

ğŸ¯ Objectives

    Scrape blog articles from the BeyondChats website

    Identify and store the oldest available blog articles

    Persist article data in a database

    Provide RESTful CRUD APIs to manage articles

    Ensure the system is modular and extensible for future phases

ğŸŒ Data Source

Blogs URL: https://beyondchats.com/blogs/

âš™ï¸ Technology Stack
Layer	                Technology
Runtime	                Node.js
Backend Framework	    Express.js
Database	            MongoDB (Atlas)
ORM	                    Mongoose
Scraping	            Axios + Cheerio
Environment Config	    dotenv

ğŸ§© Architecture Diagram
+-----------------------+
| BeyondChats Blog Site |
| https://beyondchats   |
+----------+------------+
           |
           |  HTTP Request (Axios)
           v
+-----------------------+
| Blog Scraper Script   |
| (Cheerio-based)       |
| src/scripts/          |
+----------+------------+
           |
           | Parsed Article Metadata
           v
+-----------------------+
| MongoDB Database      |
| Articles Collection  |
+----------+------------+
           |
           | REST APIs
           v
+-----------------------+
| Express API Server    |
| /api/articles         |
+-----------------------+
           |
           | JSON Responses
           v
+-----------------------+
| API Consumers         |
| (Postman / Frontend) |
+-----------------------+

ğŸ”„ Data Flow Explanation

1. he scraper script sends an HTTP request to the BeyondChats blogs page.

2. Blog article links are extracted from the statically available HTML.

3. The oldest available articles are identified programmatically.

4. Extracted article metadata (title, URL, placeholder content) is stored in MongoDB.

5. The Express server exposes CRUD APIs to:

    Create articles

    Retrieve all articles

    Retrieve a single article

    Update an article

    Delete an article

6. APIs return JSON responses consumable by tools like Postman or a frontend application.

ğŸ“ Scraping Note (Important)

The BeyondChats blog is built using client-side rendering (CSR).
When accessed via static HTTP requests (Axios + Cheerio), only a limited subset of blog links is available in the server-rendered HTML.

For Phase 1, the scraper reliably extracts and stores all statically accessible blog articles available at runtime (2 articles at the time of development), which represent the oldest accessible entries.

This approach ensures:

    Deterministic scraping

    Stable backend behavior

    Clear separation of concerns between phases

Phase 2 focuses on content enrichment and automation using Google Search results and LLMs, independent of the initial article count.

ğŸ”— API Endpoints
Method	        Endpoint	            Description
POST	        /api/articles	        Create a new article
GET	            /api/articles	        Fetch all articles
GET	            /api/articles/:id	    Fetch article by ID
PUT	            /api/articles/:id	    Update article
DELETE	        /api/articles/:id	    Delete article

ğŸ›  Local Setup Instructions
1ï¸âƒ£ Clone Repository
git clone https://github.com/charankrishnask/beyondchats-blog-automation
cd beyondchats-assignment
2ï¸âƒ£ Install Dependencies
npm install

3ï¸âƒ£ Configure Environment Variables

Create a .env file:

PORT=5000
MONGO_URI=your_mongodb_connection_string

4ï¸âƒ£ Run Scraper
node src/scripts/scrapeBlogs.js

5ï¸âƒ£ Start Server
npm run dev

API will be available at:

https://beyondchats-blog-automation-4.onrender.com/api/articles

âœ… Phase 1 Completion Status

âœ” Blog scraping implemented

âœ” Articles stored in database

âœ” CRUD APIs functional

âœ” Clean modular architecture

âœ” Ready for Phase 2 automation


ğŸš€ Next Phase

Phase 2 will:

Fetch articles via APIs

Search Google for competing articles

Scrape reference content

Use an LLM to enhance article quality

Publish updated articles via existing APIs

## ğŸ”— Live Links

- Frontend (Vercel): https://beyondchats-blog-automation.vercel.app/
- Backend API (Render): https://beyondchats-blog-automation-4.onrender.com/api/articles
