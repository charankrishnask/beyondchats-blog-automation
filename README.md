üìò Phase 1 ‚Äì Blog Scraping & CRUD APIs
üìå Overview

Phase 1 focuses on building a backend system that scrapes blog articles from the BeyondChats Blogs section and exposes CRUD APIs to manage those articles.

This phase establishes the data foundation that will later be enhanced and automated using external sources and LLMs in Phase 2.

üéØ Objectives

    Scrape blog articles from the BeyondChats website

    Identify and store the oldest available blog articles

    Persist article data in a database

    Provide RESTful CRUD APIs to manage articles

    Ensure the system is modular and extensible for future phases

üåê Data Source

Blogs URL: https://beyondchats.com/blogs/

‚öôÔ∏è Technology Stack
Layer	                Technology
Runtime	                Node.js
Backend Framework	    Express.js
Database	            MongoDB (Atlas)
ORM	                    Mongoose
Scraping	            Axios + Cheerio
Environment Config	    dotenv

üß© Architecture Diagram
+-----------------------+
| BeyondChats Blog Site |
| https://beyondchats   |
+----------+------------+
           |
           |  HTTP Request (Axios)
           v
+-----------------------+
| Blog Scraper Script   |
| (Cheerio-based)       |
| src/scripts/          |
+----------+------------+
           |
           | Parsed Article Metadata
           v
+-----------------------+
| MongoDB Database      |
| Articles Collection  |
+----------+------------+
           |
           | REST APIs
           v
+-----------------------+
| Express API Server    |
| /api/articles         |
+-----------------------+
           |
           | JSON Responses
           v
+-----------------------+
| API Consumers         |
| (Postman / Frontend) |
+-----------------------+

üîÑ Data Flow Explanation

1. he scraper script sends an HTTP request to the BeyondChats blogs page.

2. Blog article links are extracted from the statically available HTML.

3. The oldest available articles are identified programmatically.

4. Extracted article metadata (title, URL, placeholder content) is stored in MongoDB.

5. The Express server exposes CRUD APIs to:

    Create articles

    Retrieve all articles

    Retrieve a single article

    Update an article

    Delete an article

6. APIs return JSON responses consumable by tools like Postman or a frontend application.

üìù Scraping Note (Important)

The BeyondChats blog is built using client-side rendering (CSR).
When accessed via static HTTP requests (Axios + Cheerio), only a limited subset of blog links is available in the server-rendered HTML.

For Phase 1, the scraper reliably extracts and stores all statically accessible blog articles available at runtime (2 articles at the time of development), which represent the oldest accessible entries.

This approach ensures:

    Deterministic scraping

    Stable backend behavior

    Clear separation of concerns between phases

Phase 2 focuses on content enrichment and automation using Google Search results and LLMs, independent of the initial article count.

üîó API Endpoints
Method	        Endpoint	            Description
POST	        /api/articles	        Create a new article
GET	            /api/articles	        Fetch all articles
GET	            /api/articles/:id	    Fetch article by ID
PUT	            /api/articles/:id	    Update article
DELETE	        /api/articles/:id	    Delete article

üõ† Local Setup Instructions
1Ô∏è‚É£ Clone Repository
git clone 
cd beyondchats-assignment
2Ô∏è‚É£ Install Dependencies
npm install

3Ô∏è‚É£ Configure Environment Variables

Create a .env file:

PORT=5000
MONGO_URI=your_mongodb_connection_string

4Ô∏è‚É£ Run Scraper
node src/scripts/scrapeBlogs.js

5Ô∏è‚É£ Start Server
npm run dev

API will be available at:

http://localhost:5000/api/articles

‚úÖ Phase 1 Completion Status

‚úî Blog scraping implemented

‚úî Articles stored in database

‚úî CRUD APIs functional

‚úî Clean modular architecture

‚úî Ready for Phase 2 automation


üöÄ Next Phase

Phase 2 will:

Fetch articles via APIs

Search Google for competing articles

Scrape reference content

Use an LLM to enhance article quality

Publish updated articles via existing APIs